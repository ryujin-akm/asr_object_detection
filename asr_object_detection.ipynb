{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Teqcq6l80TnO"
      },
      "source": [
        "Object Detection\n",
        "https://huggingface.co/fcakyon/yolov5n-v7.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6eqGTEmyuZjJ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing libraries_required.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile libraries_required.py\n",
        "\n",
        "#Object Detection\n",
        "%pip install -U yolov5\n",
        "\n",
        "import yolov5\n",
        "\n",
        "#Speech Transcribe \n",
        "%pip install -q omegaconf torchaudio pydub\n",
        "\n",
        "import os\n",
        "from os.path import exists\n",
        "\n",
        "if not exists('silero-models'):\n",
        "  %git clone -q --depth 1 https://github.com/snakers4/silero-models\n",
        "\n",
        "%cd silero-models\n",
        "\n",
        "# silero imports\n",
        "import torch\n",
        "import random\n",
        "from glob import glob\n",
        "from omegaconf import OmegaConf\n",
        "from src.silero.utils import (init_jit_model, \n",
        "                       split_into_batches,\n",
        "                       read_audio,\n",
        "                       read_batch,\n",
        "                       prepare_model_input)\n",
        "from colab_utils import (record_audio,\n",
        "                         audio_bytes_to_np,\n",
        "                         upload_audio)\n",
        "\n",
        "# imports for uploading/recording\n",
        "import numpy as np\n",
        "import ipywidgets as widgets\n",
        "from scipy.io import wavfile\n",
        "from IPython.display import Audio, display, clear_output\n",
        "from torchaudio.functional import vad\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "YrCjuRgNvEOG",
        "outputId": "c200485d-c03b-4e40-d7db-71179b64fea4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing object_detection.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile object_detection.py\n",
        "\n",
        "# load model\n",
        "model = yolov5.load('fcakyon/yolov5s-v7.0')\n",
        "  \n",
        "# set model parameters\n",
        "model.conf = 0.25  # NMS confidence threshold\n",
        "model.iou = 0.45  # NMS IoU threshold\n",
        "model.agnostic = False  # NMS class-agnostic\n",
        "model.multi_label = False  # NMS multiple labels per box\n",
        "model.max_det = 1000  # maximum number of detections per image\n",
        "\n",
        "# set image\n",
        "img = 'https://media.istockphoto.com/id/618546412/photo/pizza-and-hamburger-on-wooden-background.jpg?s=612x612&w=0&k=20&c=vunadZByjJhV_S7DqjlmpM9hQMk_Y7dVrNt9pYCu_Ww='\n",
        "\n",
        "# perform inference\n",
        "results = model(img)\n",
        "\n",
        "# inference with larger input size\n",
        "results = model(img, size=640)\n",
        "\n",
        "# inference with test time augmentation\n",
        "results = model(img, augment=True)\n",
        "\n",
        "# parse results\n",
        "predictions = results.pred[0]\n",
        "boxes = predictions[:, :4] # x1, y1, x2, y2\n",
        "scores = predictions[:, 4]\n",
        "categories = predictions[:, 5]\n",
        "\n",
        "# show detection bounding boxes on image\n",
        "results.show()\n",
        "\n",
        "# save results into \"results/\" folder\n",
        "results.save(save_dir='results/')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0pQGCXQwZjR"
      },
      "source": [
        "###Speech to Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCW-xvckaO8s"
      },
      "source": [
        "# Live Colab Example\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-09-12T09:39:54.418700Z",
          "start_time": "2020-09-12T09:39:54.415303Z"
        },
        "id": "16gWjoEUXOhl"
      },
      "source": [
        "## Dependencies and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jc7ZqfooYZnD",
        "outputId": "129a5541-f4ad-4165-b8ed-18e68f78cc66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Writing speeech_transcribe.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile speeech_transcribe.py\n",
        "\n",
        "\n",
        "device = torch.device('cpu')   # you can use any pytorch device\n",
        "models = OmegaConf.load('models.yml')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# wav to text method\n",
        "def wav_to_text(f='test.wav'):\n",
        "  batch = read_batch([f])\n",
        "  input = prepare_model_input(batch, device=device)\n",
        "  output = model(input)\n",
        "  return decoder(output[0].cpu())\n",
        "\n",
        "# Transcribe\n",
        "\n",
        "#@markdown { run: \"auto\" }\n",
        "\n",
        "language = \"English\" #@param [\"English\", \"German\", \"Spanish\"]\n",
        "\n",
        "print(language)\n",
        "if language == 'German':\n",
        "  model, decoder = init_jit_model(models.stt_models.de.latest.jit, device=device)\n",
        "elif language == \"Spanish\":\n",
        "  model, decoder = init_jit_model(models.stt_models.es.latest.jit, device=device)\n",
        "else:\n",
        "  model, decoder = init_jit_model(models.stt_models.en.latest.jit, device=device)\n",
        "\n",
        "\n",
        "#@markdown { run: \"auto\" }\n",
        "\n",
        "use_VAD = \"No\" #@param [\"Yes\", \"No\"]\n",
        "\n",
        "#@markdown Either record audio from microphone or upload audio from file (.mp3 or .wav) { run: \"auto\" }\n",
        "\n",
        "record_or_upload = \"Record\" #@param [\"Record\", \"Upload (.mp3 or .wav)\"]\n",
        "record_seconds =   4#@param {type:\"number\", min:1, max:10, step:1}\n",
        "sample_rate = 16000\n",
        "\n",
        "def _apply_vad(audio, boot_time=0, trigger_level=9, **kwargs):\n",
        "  print('\\nVAD applied\\n')\n",
        "  vad_kwargs = dict(locals().copy(), **kwargs)\n",
        "  vad_kwargs['sample_rate'] = sample_rate\n",
        "  del vad_kwargs['kwargs'], vad_kwargs['audio']\n",
        "  audio = vad(torch.flip(audio, ([0])), **vad_kwargs)\n",
        "  return vad(torch.flip(audio, ([0])), **vad_kwargs)\n",
        "\n",
        "def _recognize(audio):\n",
        "  display(Audio(audio, rate=sample_rate, autoplay=True))\n",
        "  if use_VAD == \"Yes\":\n",
        "    audio = _apply_vad(audio)\n",
        "  wavfile.write('test.wav', sample_rate, (32767*audio).numpy().astype(np.int16))\n",
        "  transcription = wav_to_text()\n",
        "  print('\\n\\nTRANSCRIPTION:\\n')\n",
        "  print(transcription)\n",
        "\n",
        "def _record_audio(b):\n",
        "  clear_output()\n",
        "  audio = record_audio(record_seconds)\n",
        "  wavfile.write('recorded.wav', sample_rate, (32767*audio).numpy().astype(np.int16))\n",
        "  _recognize(audio)\n",
        "\n",
        "def _upload_audio(b):\n",
        "  clear_output()\n",
        "  audio = upload_audio()\n",
        "  _recognize(audio)\n",
        "  return audio\n",
        "\n",
        "if record_or_upload == \"Record\":\n",
        "  button = widgets.Button(description=\"Record Speech\")\n",
        "  button.on_click(_record_audio)\n",
        "  display(button)\n",
        "else:\n",
        "  audio = _upload_audio(\"\")\n",
        "\n",
        "\n",
        "#@markdown Check audio after applying VAD { run: \"auto\" }\n",
        "\n",
        "if record_or_upload == \"Record\":\n",
        "  audio = read_audio('recorded.wav', sample_rate)\n",
        "display(Audio(_apply_vad(audio), rate=sample_rate, autoplay=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6QIeg7XffsO"
      },
      "source": [
        "## Transcribe"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2853f7ba28cd4f978ddb0223f94f6ebe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Record Speech",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_d2e686db1f474dd388034ba5a7d7ebe5",
            "style": "IPY_MODEL_8fa0cb67a12746748901a32896e09954",
            "tooltip": ""
          }
        },
        "8fa0cb67a12746748901a32896e09954": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ButtonStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "d2e686db1f474dd388034ba5a7d7ebe5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
